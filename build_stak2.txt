extract  https://github.com/fireice-uk/xmr-stak-dep/releases/download/v1/xmr-stak-dep.zip to c:\xmr-stak-dep  *create dir

run as admininstrator CMD prompt
extract xmr-stak source into c:\users(logonprofilename)\

cd to that source dir

"D:\Program Files\vs\Common7\Tools\VsMSBuildCmd.bat"mkdir build

cd build

set CMAKE_PREFIX_PATH=C:\xmr-stak-dep\hwloc;C:\xmr-stak-dep\libmicrohttpd;C:\xmr-stak-dep\openssl



cmake .. -DCUDA_ARCH=20 -DCMAKE_LINK_STATIC=ON -DMICROHTTPD_ENABLE=OFF -DCPU_ENABLE=ON -DOpenCL_ENABLE=ON -DXMR-STAK_LARGEGRID=OFF -DXMR-STAK_THREADS=64 -DOpenSSL_ENABLE=OFF -DHWLOC_ENABLE=ON
full build string as:
cmake -G "Visual Studio 15 2017 Win64" -T v140,host=x64 .. -DCUDA_ARCH=20 -DCMAKE_LINK_STATIC=OFF -DMICROHTTPD_ENABLE=OFF -DCPU_ENABLE=ON -DOpenCL_ENABLE=ON -DXMR-STAK_LARGEGRID=OFF -DXMR-STAK_THREADS=64 -DOpenSSL_ENABLE=OFF -DHWLOC_ENABLE=ON -DCMAKE_LINK_STATIC=ON


possible switch -DCMAKE_LINK_STATIC=ON for all machine types but might be slower
and add arches 61,30,50,52,60 turn on largerid, threads of 128
re : https://github.com/fireice-uk/xmr-stak/blob/master/doc/compile.md
release version built as:

cmake -G "Visual Studio 15 2017 Win64" -T v140,host=x64 .. -DCUDA_ARCH=20;30;50;52;60;61 -DHWLOC_ENABLE=ON -DMICROHTTPD_ENABLE=OFF -DCPU_ENABLE=ON -DOpenCL_ENABLE=ON -DXMR-STAK_LARGEGRID=ON -DXMR-STAK_THREADS=128 -DOpenSSL_ENABLE=OFF -DCMAKE_LINK_STATIC=ON

then at DOS prompt

cmake --build . --config Release --target install

cd bin\Release

copy C:\xmr-stak-dep\openssl\bin\* . (skip if no openssl neeed)



smx=14 for m2050 theads 54 blocks 64
smx=16 for gtx 580 64 x 64 


Notes:
Supported on CUDA 7 and later
Fermi (CUDA 3.2 until CUDA 8) (deprecated from CUDA 9):
SM20 or SM_20, compute_30 – Older cards such as GeForce 400, 500, 600, GT-630
Kepler (CUDA 5 and later):
SM30 or SM_30, compute_30 – Kepler architecture (generic – Tesla K40/K80, GeForce 700, GT-730)
Adds support for unified memory programming
SM35 or SM_35, compute_35 – More specific Tesla K40
Adds support for dynamic parallelism. Shows no real benefit over SM30 in my experience.
SM37 or SM_37, compute_37 – More specific Tesla K80
Adds a few more registers. Shows no real benefit over SM30 in my experience
Maxwell (CUDA 6 and later):
SM50 or SM_50, compute_50 – Tesla/Quadro M series
SM52 or SM_52, compute_52 – Quadro M6000 , GeForce 900, GTX-970, GTX-980, GTX Titan X
SM53 or SM_53, compute_53 – Tegra (Jetson) TX1 / Tegra X1
Pascal (CUDA 8 and later)
SM60 or SM_60, compute_60 – Quadro GP100, Tesla P100, DGX-1 (Generic Pascal)
SM61 or SM_61, compute_61 – GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4, Discrete GPU on the NVIDIA Drive PX2
SM62 or SM_62, compute_62 – Integrated GPU on the NVIDIA Drive PX2, Tegra (Jetson) TX2
Volta (CUDA 9 and later)
SM70 or SM_70, compute_70 – DGX-1 with Volta, Tesla V100, GTX 1180 (GV104), Titan V, Quadro GV100
SM72 or SM_72, compute_72 – Jetson AGX Xavier
Turing (CUDA 10 and later)
SM75 or SM_75, compute_75 – GTX Turing – GTX 1660 Ti, RTX 2060, RTX 2070, RTX 2080, Titan RTX, Quadro RTX 4000, Quadro RTX 5000, Quadro RTX 6000, Quadro RTX 8000
